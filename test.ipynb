{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce4430a8-6d92-4a06-bbb6-eef63d09a5c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selected_df = df.select(\n",
    "    \"customer_id\",\n",
    "    \"interaction_id\",\n",
    "    col(\"date_time\").alias(\"interaction_date\"),\n",
    "    \"issue_category\",\n",
    "    \"issue_description\",\n",
    "    \"agent_id\"\n",
    ")\n",
    "display(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb600ff-bf3b-4a59-9b74-4aa0c806a9b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"gen_ai_workshop.himanshu_gupta.cust_service_data\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f82a414-08c2-444a-96ab-3ca31741e994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, trim, col, rtrim\n",
    "\n",
    "# Select unique customer records\n",
    "unique_df = df.select(\n",
    "    \"customer_id\", \"name\", \"email\", \"phone_number\", \"address\"\n",
    ").dropDuplicates([\"customer_id\", \"name\", \"email\", \"phone_number\", \"address\"])\n",
    "\n",
    "display(unique_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2780a15c-ddd6-40de-aba7-f28b9848f62f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_agents_df = df.select(\"agent_id\").distinct()\n",
    "display(unique_agents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23faefd5-65cd-42e2-9845-4962047076ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit, col\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Generate 50 unique agent names\n",
    "agent_names_list = [f\"Agent_{i+1}\" for i in range(50)]\n",
    "\n",
    "# Assign a unique agent name to each agent_id\n",
    "agent_ids = [row.agent_id for row in unique_agents_df.collect()]\n",
    "unique_names = agent_names_list[:len(agent_ids)]\n",
    "\n",
    "agent_info = [Row(agent_id=agent_id, agent_name=agent_name) for agent_id, agent_name in zip(agent_ids, unique_names)]\n",
    "agent_info_df = spark.createDataFrame(agent_info)\n",
    "\n",
    "fake_agents_df = agent_info_df.withColumn(\n",
    "    \"agent_email\", concat(lit(\"agent_\"), col(\"agent_name\"), lit(\"@example.com\"))\n",
    ")\n",
    "\n",
    "display(fake_agents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4b50bf-4073-4cd0-aa94-8555da1ac781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, when, round as spark_round, monotonically_increasing_id\n",
    "\n",
    "# Generate survey responses for each interaction\n",
    "survey_df = df.select(\n",
    "    \"interaction_id\",\n",
    ").withColumn(\n",
    "    \"survey_id\", monotonically_increasing_id()\n",
    ").withColumn(\n",
    "    \"satisfaction_rating\", (spark_round(rand() * 4) + 1).cast(\"integer\")  # Ratings 1-5\n",
    ").withColumn(\n",
    "    \"would_recommend\", when(rand() > 0.2, \"Yes\").otherwise(\"No\")\n",
    ").withColumn(\n",
    "    \"survey_comment\",\n",
    "    when(col(\"satisfaction_rating\") >= 4, \"Great service!\")\n",
    "    .when(col(\"satisfaction_rating\") == 3, \"Average experience.\")\n",
    "    .otherwise(\"Needs improvement.\")\n",
    ")\n",
    "\n",
    "display(survey_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
