{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5fcd388-6651-4d85-8f75-dbff8aaa48eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daaa3cfa-4aff-416b-87e2-f920f90ce084",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Running Configuration File to setup default catalog and schema"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./00_Config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7bea3f5-9d43-4440-a4f8-0627f27dff63",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create customer service table"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the customer service data CSV file\n",
    "csv_path = f\"/Volumes/{catalog_name}/default/data/cust_service_data.csv\"\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame with headers\n",
    "df = spark.read.csv(csv_path, header=True)\n",
    "\n",
    "# Define the table name using the user's schema\n",
    "table_name = f\"{schema_name}.cust_service_data\"\n",
    "\n",
    "# Overwrite and save the DataFrame as a table in the user's schema\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Display the contents of the newly created table\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba380cd-244f-4b5f-8a97-de701bde9d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create policies table"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the policies data CSV file\n",
    "csv_path = f\"/Volumes/{catalog_name}/default/data/policies.csv\"\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame with headers\n",
    "df = spark.read.csv(csv_path, header=True)\n",
    "\n",
    "# Define the table name using the user's schema\n",
    "table_name = f\"{schema_name}.policies\"\n",
    "\n",
    "# Overwrite and save the DataFrame as a table in the user's schema\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Display the contents of the newly created table\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d6d3b2-e77c-481b-883a-ac4c982ba1e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create product docs table"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the product documentation CSV file\n",
    "csv_path = f\"/Volumes/{catalog_name}/default/data/product_docs.csv\"\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame with headers\n",
    "df = spark.read.csv(csv_path, header=True)\n",
    "\n",
    "# Define the table name using the user's schema\n",
    "table_name = f\"{schema_name}.product_docs\"\n",
    "\n",
    "# Overwrite and save the DataFrame as a table in the user's schema\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Display the contents of the newly created table\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1ada95-14d3-4b2f-85b6-d64cbc7cdc7b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create customer table"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the product documentation CSV file\n",
    "csv_path = f\"/Volumes/{catalog_name}/default/data/customer.csv\"\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame with headers\n",
    "df = spark.read.csv(csv_path, header=True)\n",
    "\n",
    "# Define the table name using the user's schema\n",
    "table_name = f\"{schema_name}.customer\"\n",
    "\n",
    "# Overwrite and save the DataFrame as a table in the user's schema\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Display the contents of the newly created table\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9709096a-6b0f-4ee5-9b21-60ac481e8c18",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Agent table"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the product documentation CSV file\n",
    "csv_path = f\"/Volumes/{catalog_name}/default/data/agent.csv\"\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame with headers\n",
    "df = spark.read.csv(csv_path, header=True)\n",
    "\n",
    "# Define the table name using the user's schema\n",
    "table_name = f\"{schema_name}.agent\"\n",
    "\n",
    "# Overwrite and save the DataFrame as a table in the user's schema\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Display the contents of the newly created table\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f40a9d-24bc-4719-a0f0-d9f5dfff6da0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Survey table"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the product documentation CSV file\n",
    "csv_path = f\"/Volumes/{catalog_name}/default/data/survey.csv\"\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame with headers\n",
    "df = spark.read.csv(csv_path, header=True)\n",
    "\n",
    "# Define the table name using the user's schema\n",
    "table_name = f\"{schema_name}.survey\"\n",
    "\n",
    "# Overwrite and save the DataFrame as a table in the user's schema\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Display the contents of the newly created table\n",
    "display(spark.table(table_name))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_Load_Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
