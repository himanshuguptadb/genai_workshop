{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5fcd388-6651-4d85-8f75-dbff8aaa48eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daaa3cfa-4aff-416b-87e2-f920f90ce084",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Running Configuration File to setup default catalog"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./00_Config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bdd812a-ccb2-4c75-a278-f29303edc761",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Capturing username to create unique schema name"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "#import yaml\n",
    "import os\n",
    "\n",
    "# Use the workspace client to retrieve information about the current user\n",
    "w = WorkspaceClient()\n",
    "user_email = w.current_user.me().display_name\n",
    "username = user_email.split(\"@\")[0].replace(\" \", \"_\")\n",
    "\n",
    "# Catalog and schema have been automatically created thanks to lab environment\n",
    "schema_name = f\"{username}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cca0cb5-0659-40d6-848d-57846ba66357",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating schema for the user"
    }
   },
   "outputs": [],
   "source": [
    "# Create the schema if it does not already exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7bea3f5-9d43-4440-a4f8-0627f27dff63",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create customer service table"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the customer service data CSV file\n",
    "csv_path = f\"/Volumes/{catalog_name}/default/data/cust_service_data.csv\"\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame with headers\n",
    "df = spark.read.csv(csv_path, header=True)\n",
    "\n",
    "# Define the table name using the user's schema\n",
    "table_name = f\"{schema_name}.cust_service_data\"\n",
    "\n",
    "# Overwrite and save the DataFrame as a table in the user's schema\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Display the contents of the newly created table\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba380cd-244f-4b5f-8a97-de701bde9d1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create policies table"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the policies data CSV file\n",
    "csv_path = f\"/Volumes/{catalog_name}/default/data/policies.csv\"\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame with headers\n",
    "df = spark.read.csv(csv_path, header=True)\n",
    "\n",
    "# Define the table name using the user's schema\n",
    "table_name = f\"{schema_name}.policies\"\n",
    "\n",
    "# Overwrite and save the DataFrame as a table in the user's schema\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Display the contents of the newly created table\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d6d3b2-e77c-481b-883a-ac4c982ba1e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create product docs table"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path to the product documentation CSV file\n",
    "csv_path = f\"/Volumes/{catalog_name}/default/data/product_docs.csv\"\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame with headers\n",
    "df = spark.read.csv(csv_path, header=True)\n",
    "\n",
    "# Define the table name using the user's schema\n",
    "table_name = f\"{schema_name}.product_docs\"\n",
    "\n",
    "# Overwrite and save the DataFrame as a table in the user's schema\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Display the contents of the newly created table\n",
    "display(spark.table(table_name))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_Load_Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
